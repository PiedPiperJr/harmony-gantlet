{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, F = 100, 20, 11 # n_samples, sequence lenght, number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generer les données dvec plus d'aleatoire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from string import ascii_lowercase\n",
    "from pathlib import Path\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "\n",
    "sandbox = [\n",
    "    lambda: float(stats.poisson.rvs(mu=np.random.uniform(100, 300))),\n",
    "    lambda: float(stats.nbinom.rvs(n=np.random.uniform(10, 50), \n",
    "                                    p=np.random.uniform(0.1, 0.9))),\n",
    "    lambda: float(stats.randint.rvs(low=100, high=800)),\n",
    "    lambda: float(np.floor(stats.norm.rvs(loc=np.random.uniform(100, 500), \n",
    "                                            scale=np.random.uniform(50, 200)))),\n",
    "    lambda: float(np.ceil(stats.uniform.rvs(loc=100, scale=700))),\n",
    "    lambda: float(np.abs(int(stats.norm.rvs(loc=300, scale=100)) * \n",
    "                        int(stats.uniform.rvs(loc=1, scale=2)))),\n",
    "    lambda: float(np.floor((stats.norm.rvs(loc=200, scale=50) / \n",
    "                            stats.norm.rvs(loc=100, scale=25)) * 100)),\n",
    "    lambda: float(np.random.randint(100, 800) % float(np.random.uniform(10, 50)) + 100),\n",
    "    lambda: float(stats.binom.rvs(n=500, p=np.random.uniform(0.1, 0.5))),\n",
    "    lambda: float(stats.geom.rvs(p=np.random.uniform(0.1, 0.5))),\n",
    "    lambda: float(np.power(np.random.randint(2, 10), \n",
    "                            np.random.randint(1, 3)) % 700 + 100),\n",
    "    lambda: float(np.abs(np.sin(np.random.uniform(0, np.pi)) * 500 + 300)),\n",
    "    lambda: float(np.random.choice([\n",
    "        101, 103, 107, 109, 113, 127, 131, 137, 139, \n",
    "        149, 151, 157, 163, 167, 173, 179, 181, 191, \n",
    "        193, 197, 199\n",
    "    ]) + np.random.randint(-50, 50)),\n",
    "    lambda: float(np.log(np.random.randint(10, 1000)) * 50 + 100),\n",
    "    lambda: float(np.abs(np.random.normal(0, 1) ** np.random.uniform(1, 3)) * 100 % 700 + 100),\n",
    "    lambda: float(special.factorial(np.random.randint(3, 6)) % 700 + 100),\n",
    "    lambda: float(np.gcd(np.random.randint(100, 800), np.random.randint(100, 800))),\n",
    "    lambda: float(np.lcm(np.random.randint(10, 100), np.random.randint(10, 100)) % 700 + 100)\n",
    "]\n",
    "\n",
    "\n",
    "letter_strategies = {\n",
    "    letter: np.random.choice(sandbox, size=F, replace=False).tolist()\n",
    "    for letter in ascii_lowercase\n",
    "}\n",
    "\n",
    "def generate_advanced_timeseries(N=N, T=T, F=F, distribution_strategies = letter_strategies):\n",
    "    base_dir = f\"{N} {T} {F}\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    data = []\n",
    "\n",
    "    for letter, strategies in distribution_strategies.items():\n",
    "        letter_file = Path(base_dir, f\"letter_{letter}.json\")\n",
    "        letter_data = []\n",
    "\n",
    "        for _ in range(N):\n",
    "            sample = [\n",
    "                [int(max(100, min(800, strategy()))) for strategy in strategies]\n",
    "                for _ in range(T)\n",
    "            ]\n",
    "            letter_data.append(sample)\n",
    "\n",
    "        data.append(letter_data)\n",
    "        letter_file.write_text(json.dumps(letter_data))\n",
    "\n",
    "    dataset = Path(base_dir, f\"dataset.json\")\n",
    "    dataset.write_text(json.dumps(data))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the advanced dataset\n",
    "generated_data = generate_advanced_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[100, 118, 759, 100, 245, 405, 298, 447, 100, 240, 742],\n",
       "  [100, 102, 756, 108, 100, 362, 210, 392, 100, 196, 546],\n",
       "  [100, 203, 718, 100, 188, 366, 298, 425, 100, 268, 446],\n",
       "  [100, 206, 202, 201, 104, 414, 421, 297, 100, 271, 436],\n",
       "  [100, 100, 777, 176, 158, 402, 268, 232, 100, 203, 506],\n",
       "  [100, 140, 297, 100, 298, 426, 592, 708, 100, 117, 328],\n",
       "  [100, 109, 561, 133, 125, 402, 488, 124, 100, 271, 196],\n",
       "  [100, 206, 338, 119, 448, 399, 226, 262, 100, 296, 732],\n",
       "  [100, 107, 707, 100, 129, 361, 172, 550, 100, 266, 414],\n",
       "  [100, 164, 167, 100, 245, 397, 330, 614, 100, 151, 736],\n",
       "  [100, 134, 721, 100, 444, 399, 190, 390, 100, 205, 217],\n",
       "  [100, 112, 613, 100, 529, 380, 167, 506, 100, 268, 199],\n",
       "  [100, 184, 389, 100, 312, 393, 483, 299, 100, 108, 800],\n",
       "  [100, 116, 752, 100, 614, 429, 390, 594, 100, 100, 157],\n",
       "  [100, 100, 282, 118, 112, 440, 157, 308, 100, 224, 309],\n",
       "  [100, 197, 711, 100, 101, 431, 456, 190, 100, 186, 165],\n",
       "  [100, 100, 133, 100, 148, 440, 454, 706, 100, 269, 310],\n",
       "  [100, 106, 310, 100, 649, 322, 272, 296, 100, 189, 800],\n",
       "  [100, 186, 159, 100, 114, 383, 171, 247, 100, 270, 381],\n",
       "  [100, 213, 641, 100, 112, 442, 162, 663, 100, 116, 766]],\n",
       " 'a')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(f\"{N} {T} {F}\")\n",
    "dataset = {\"x\": [], \"y\":[]}\n",
    "for letter in ascii_lowercase:\n",
    "    letter_file = data_path / f\"letter_{letter}.json\"\n",
    "    letter_data = json.loads(letter_file.read_text())\n",
    "    dataset[\"x\"].extend(letter_data)\n",
    "    dataset[\"y\"].extend([letter for i in range(N)])\n",
    "\n",
    "dataset[\"x\"][0], dataset[\"y\"][0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert data and labels to numpy arrays\n",
    "X = np.array(dataset[\"x\"])\n",
    "y = np.array(dataset[\"y\"])\n",
    "\n",
    "# One-hot encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_one_hot = tf.keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_normalized = np.zeros_like(X, dtype=float)\n",
    "for i in range(X.shape[0]):\n",
    "    X_normalized[i] = scaler.fit_transform(X[i])\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_normalized, y_one_hot, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Build the model\n",
    "def create_time_series_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        # LSTM layers for sequence processing\n",
    "        tf.keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get input shape\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "num_classes = y_one_hot.shape[1]\n",
    "\n",
    "# Create and train the model\n",
    "model = create_time_series_model(input_shape, num_classes)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Optional: Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nPrediction Details:\")\n",
    "for pred, true in zip(predicted_classes, true_classes):\n",
    "    print(f\"Predicted: {label_encoder.inverse_transform([pred])[0]}, \"\n",
    "          f\"True: {label_encoder.inverse_transform([true])[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"mode.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 11)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step\n"
     ]
    }
   ],
   "source": [
    "_model = tf.keras.models.load_model(f\"{N} {T} {F}/[0] model.keras\")\n",
    "pred = _model.predict(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights exported to model.weights.json\n",
      "Model architecture exported to model_architecture.json\n",
      "Model weights exported to model.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/read_weights.py:28: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.uint8, np.uint16, np.object, np.bool]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/harmony-gloves/main.ipynb Cellule 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m export_weights_to_json(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m export_full_model(model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m export_to_tfjs(model)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Loading methods\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Load weights from JSON\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mload_weights_from_json\u001b[39m(model, filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel.weights.json\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m/workspaces/harmony-gloves/main.ipynb Cellule 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mexport_to_tfjs\u001b[39m(model, output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtfjs_model\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtfjs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     tfjs\u001b[39m.\u001b[39mconverters\u001b[39m.\u001b[39msave_keras_model(model, output_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bprobable-barnacle-75gw5j75gqvhpgr6/workspaces/harmony-gloves/main.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel exported to TensorFlow.js format in \u001b[39m\u001b[39m{\u001b[39;00moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__future__\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m converters\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m quantization\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m version\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/converters/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__future__\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-imports,line-too-long\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverter\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m convert\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras_h5_conversion\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m save_keras_model\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras_tfjs_loader\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m deserialize_keras_model\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/converters/converter.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m version\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m common\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m keras_h5_conversion \u001b[39mas\u001b[39;00m conversion\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m keras_tfjs_loader\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m tf_saved_model_conversion_v2\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/converters/keras_h5_conversion.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mh5py\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m write_weights  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconverters\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m common\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnormalize_weight_name\u001b[39m(weight_name):\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/write_weights.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflow\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtf\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m quantization\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m read_weights\n\u001b[1;32m     27\u001b[0m _OUTPUT_DTYPES \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat16, np\u001b[39m.\u001b[39mfloat32, np\u001b[39m.\u001b[39mint32, np\u001b[39m.\u001b[39mcomplex64,\n\u001b[1;32m     28\u001b[0m                   np\u001b[39m.\u001b[39muint8, np\u001b[39m.\u001b[39muint16, np\u001b[39m.\u001b[39mbool, np\u001b[39m.\u001b[39mobject]\n\u001b[1;32m     29\u001b[0m _AUTO_DTYPE_CONVERSION \u001b[39m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     np\u001b[39m.\u001b[39mdtype(np\u001b[39m.\u001b[39mfloat16): np\u001b[39m.\u001b[39mfloat32,\n\u001b[1;32m     31\u001b[0m     np\u001b[39m.\u001b[39mdtype(np\u001b[39m.\u001b[39mfloat64): np\u001b[39m.\u001b[39mfloat32,\n\u001b[1;32m     32\u001b[0m     np\u001b[39m.\u001b[39mdtype(np\u001b[39m.\u001b[39mint64): np\u001b[39m.\u001b[39mint32,\n\u001b[1;32m     33\u001b[0m     np\u001b[39m.\u001b[39mdtype(np\u001b[39m.\u001b[39mcomplex128): np\u001b[39m.\u001b[39mcomplex64}\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/tensorflowjs/read_weights.py:28\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtensorflowjs\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m quantization\n\u001b[1;32m     27\u001b[0m _INPUT_DTYPES \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat16, np\u001b[39m.\u001b[39mfloat32, np\u001b[39m.\u001b[39mint32, np\u001b[39m.\u001b[39mcomplex64,\n\u001b[0;32m---> 28\u001b[0m                  np\u001b[39m.\u001b[39muint8, np\u001b[39m.\u001b[39muint16, np\u001b[39m.\u001b[39;49mobject, np\u001b[39m.\u001b[39mbool]\n\u001b[1;32m     30\u001b[0m \u001b[39m# Number of bytes used to encode the length of a string in a string tensor.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m STRING_LENGTH_NUM_BYTES \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[0;32m/workspaces/harmony-gloves/venv/lib/python3.12/site-packages/numpy/__init__.py:394\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    389\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    393\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 394\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __expired_attributes__:\n\u001b[1;32m    397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` was removed in the NumPy 2.0 release. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m__expired_attributes__[attr]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'model' is your trained TensorFlow model from the previous script\n",
    "\n",
    "# Export weights to JSON\n",
    "def export_weights_to_json(model, filename='model.weights.json'):\n",
    "    weights = {}\n",
    "    for layer in model.layers:\n",
    "        layer_weights = layer.get_weights()\n",
    "        if layer_weights:  # Only store layers with weights\n",
    "            weights[layer.name] = [w.tolist() for w in layer_weights]\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(weights, f)\n",
    "    print(f\"Weights exported to {filename}\")\n",
    "\n",
    "# Export model architecture and weights\n",
    "def export_full_model(model, architecture_filename='model_architecture.json', \n",
    "                      weights_filename='model.weights.h5'):\n",
    "    # Save model architecture as JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(architecture_filename, 'w') as f:\n",
    "        f.write(model_json)\n",
    "    \n",
    "    # Save weights in HDF5 format\n",
    "    model.save_weights(weights_filename)\n",
    "    print(f\"Model architecture exported to {architecture_filename}\")\n",
    "    print(f\"Model weights exported to {weights_filename}\")\n",
    "\n",
    "# TensorFlow.js export\n",
    "def export_to_tfjs(model, output_dir='tfjs_model'):\n",
    "    import tensorflowjs as tfjs\n",
    "    tfjs.converters.save_keras_model(model, output_dir)\n",
    "    print(f\"Model exported to TensorFlow.js format in {output_dir}\")\n",
    "\n",
    "# Demonstrate usage\n",
    "export_weights_to_json(model)\n",
    "export_full_model(model)\n",
    "export_to_tfjs(model)\n",
    "\n",
    "# Loading methods\n",
    "\n",
    "# Load weights from JSON\n",
    "def load_weights_from_json(model, filename='model.weights.json'):\n",
    "    with open(filename, 'r') as f:\n",
    "        weights_dict = json.load(f)\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if layer.name in weights_dict:\n",
    "            # Convert back to numpy arrays\n",
    "            layer_weights = [np.array(w) for w in weights_dict[layer.name]]\n",
    "            layer.set_weights(layer_weights)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load TensorFlow model\n",
    "def load_full_model(architecture_filename='model_architecture.json', \n",
    "                    weights_filename='model.weights.h5'):\n",
    "    # Load model architecture\n",
    "    with open(architecture_filename, 'r') as f:\n",
    "        model_json = f.read()\n",
    "    \n",
    "    loaded_model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Load weights\n",
    "    loaded_model.load_weights(weights_filename)\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# TensorFlow.js loading example (in JavaScript)\n",
    "\"\"\"\n",
    "// JavaScript code for loading TensorFlow.js model\n",
    "import * as tf from '@tensorflow/tfjs';\n",
    "\n",
    "async function loadModel() {\n",
    "    const model = await tf.loadLayersModel('path/to/tfjs_model/model.json');\n",
    "    return model;\n",
    "}\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
