{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, F = 10, 7, 11 # n_samples, sequence lenght, number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104312"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from string import ascii_lowercase\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# n est le nombre d'individus par lettre\n",
    "# p est le nombre de snapshot par  (nombre de capture dans un delta t)\n",
    "# m est le nombre de variables qu'on a (5 flex + 6 accelérations données par le gyroscope:: Les accelerations de rotation et translation)\n",
    "\n",
    "# Create base directory for dataset\n",
    "base_dir = f\"{N} {T} {F}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "data = []\n",
    "\n",
    "for i in range(26):\n",
    "    # Create directory for each letter\n",
    "    letter = ascii_lowercase[i]\n",
    "    letter_file = Path(base_dir, f\"letter_{letter}.json\")\n",
    "    letter_data = []\n",
    "    # os.makedirs(letter_dir, exist_ok=True\n",
    "    for k in range(N):\n",
    "        sample = []\n",
    "        for j in range(T):\n",
    "            # Generate m random numbers with different distributions\n",
    "            row = []\n",
    "            \n",
    "            # Mix of normal, uniform, and other distributions for variety\n",
    "            row.extend(np.random.normal(450, 100, size=F//3).astype(int).clip(100, 800))\n",
    "            row.extend(np.random.uniform(100, 800, size=F//3).astype(int))\n",
    "            row.extend(np.random.exponential(scale=200, size=F//3 + F%3).astype(int).clip(100, 800))\n",
    "            row = [int(e) for e in row]\n",
    "            sample.append(row)\n",
    "        letter_data.append(sample)\n",
    "    data.append(letter_data)\n",
    "    letter_file.write_text(json.dumps(letter_data))\n",
    "dataset = Path(base_dir, f\"dataset.json\")\n",
    "dataset.write_text(json.dumps(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[332, 453, 609, 356, 150, 462, 165, 113, 800, 100, 127],\n",
       "  [328, 308, 397, 697, 311, 560, 534, 584, 375, 180, 100],\n",
       "  [354, 372, 357, 443, 775, 299, 129, 100, 100, 100, 190],\n",
       "  [277, 501, 300, 428, 752, 161, 161, 213, 126, 369, 173],\n",
       "  [402, 464, 271, 734, 700, 184, 100, 116, 444, 106, 100],\n",
       "  [501, 280, 331, 157, 787, 545, 126, 184, 140, 155, 109],\n",
       "  [490, 603, 314, 527, 219, 759, 155, 196, 106, 100, 176]],\n",
       " 'a')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(f\"{N} {T} {F}\")\n",
    "dataset = {\"x\": [], \"y\":[]}\n",
    "for letter in ascii_lowercase:\n",
    "    letter_file = data_path / f\"letter_{letter}.json\"\n",
    "    letter_data = json.loads(letter_file.read_text())\n",
    "    dataset[\"x\"].extend(letter_data)\n",
    "    dataset[\"y\"].extend([letter for i in range(N)])\n",
    "\n",
    "dataset[\"x\"][0], dataset[\"y\"][0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
